# Databricks Workflows Configuration
# Medallion Architecture - GM Data Analysis 2023

resources:
  jobs:
    gm_medallion_pipeline_2023:
      name: "GM Medallion Pipeline - 2023"
      description: "End-to-end medallion architecture pipeline for GM data 2023"
      
      # Use serverless compute for cost efficiency
      job_clusters:
        - job_cluster_key: "main_cluster"
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "${var.cluster_node_type}"
            num_workers: 2
            spark_conf:
              "spark.databricks.delta.preview.enabled": "true"
              "spark.databricks.delta.retentionDurationCheck.enabled": "false"
            custom_tags:
              project: "gm_analysis"
              environment: "production"
              year: "2023"
      
      # Email notifications
      email_notifications:
        on_success:
          - "data-engineering@company.com"
        on_failure:
          - "data-engineering@company.com"
          - "alerts@company.com"
      
      # Timeout and retry configuration
      timeout_seconds: 7200  # 2 hours
      max_concurrent_runs: 1
      
      tasks:
        # Task 1: Bronze Layer Ingestion
        - task_key: "bronze_ingestion"
          description: "Ingest raw data into Bronze layer"
          notebook_task:
            notebook_path: "../notebooks/bronze_ingestion.py"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
              schema: "gm_test_schema"
          job_cluster_key: "main_cluster"
          timeout_seconds: 3600
          retry_on_timeout: true
          max_retries: 2
          min_retry_interval_millis: 60000
        
        # Task 2: Silver Layer Transformation
        - task_key: "silver_transformation"
          description: "Cleanse and transform data to Silver layer"
          depends_on:
            - task_key: "bronze_ingestion"
          notebook_task:
            notebook_path: "../notebooks/silver_transformation.py"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
              schema: "gm_test_schema"
          job_cluster_key: "main_cluster"
          timeout_seconds: 3600
          retry_on_timeout: true
          max_retries: 2
          min_retry_interval_millis: 60000
        
        # Task 3: Data Quality Validation
        - task_key: "data_validation"
          description: "Validate data quality in Silver layer"
          depends_on:
            - task_key: "silver_transformation"
          notebook_task:
            notebook_path: "../notebooks/data_validation.py"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
              schema: "gm_test_schema"
          job_cluster_key: "main_cluster"
          timeout_seconds: 1800
        
        # Task 4: Gold Layer Aggregation
        - task_key: "gold_aggregation"
          description: "Create business aggregates in Gold layer"
          depends_on:
            - task_key: "data_validation"
          notebook_task:
            notebook_path: "../notebooks/gold_aggregation.py"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
              schema: "gm_test_schema"
          job_cluster_key: "main_cluster"
          timeout_seconds: 3600
          retry_on_timeout: true
          max_retries: 2
        
        # Task 5: Gold Type Casting (Optional)
        - task_key: "gold_type_casting"
          description: "Cast Gold tables to logical types (optional)"
          depends_on:
            - task_key: "gold_aggregation"
          notebook_task:
            notebook_path: "../notebooks/gold_type_casting.py"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
              schema: "gm_test_schema"
          job_cluster_key: "main_cluster"
          timeout_seconds: 1800
        
        # Task 6: Table Optimization
        - task_key: "optimize_tables"
          description: "Optimize all Delta tables"
          depends_on:
            - task_key: "gold_type_casting"
          notebook_task:
            notebook_path: "../notebooks/table_optimization.py"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
              schema: "gm_test_schema"
              run_vacuum: "false"
          job_cluster_key: "main_cluster"
          timeout_seconds: 3600
        
        # Task 7: Generate Analytics Dashboard
        - task_key: "create_dashboard"
          description: "Generate analytics visualizations"
          depends_on:
            - task_key: "optimize_tables"
          notebook_task:
            notebook_path: "/Shared/gm_analysis/02_analytics_dashboard"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
          job_cluster_key: "main_cluster"
          timeout_seconds: 1800
      
      # Schedule: Run daily at 2 AM UTC
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"  # Start paused, enable when ready
    
    # Maintenance Job - Run weekly
    gm_maintenance_job:
      name: "GM Delta Table Maintenance"
      description: "Weekly maintenance job for VACUUM and OPTIMIZE"
      
      job_clusters:
        - job_cluster_key: "maintenance_cluster"
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "${var.cluster_node_type}"
            num_workers: 2
      
      email_notifications:
        on_failure:
          - "data-engineering@company.com"
      
      tasks:
        - task_key: "vacuum_tables"
          description: "VACUUM old files from Delta tables"
          notebook_task:
            notebook_path: "../notebooks/table_optimization.py"
            base_parameters:
              year: "2023"
              catalog: "${var.catalog_name}"
              schema: "gm_test_schema"
              run_vacuum: "true"
          job_cluster_key: "maintenance_cluster"
          timeout_seconds: 7200
      
      # Schedule: Run weekly on Sunday at 3 AM UTC
      schedule:
        quartz_cron_expression: "0 0 3 ? * SUN"
        timezone_id: "UTC"
        pause_status: "PAUSED"
